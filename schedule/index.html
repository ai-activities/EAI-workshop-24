<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=default">
    </script>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>EAI-KDD'24 - Schedule</title>
</head>

<body>

    <div class="banner">
        <img src="assets/banner-2.jpg" alt="EAI-KDD’24", width="1000" height="500">
        <div class="centered">
            3rd Workshop on Ethical Artificial Intelligence: Methods and Applications <br> 
            (held in conjunction with ACM SIGKDD’24) <br> 
            August 25, 2024, Barcelona, Spain   
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a title="Workshop Home Page" href=".">Home</a>
            </td>
            <td class="navigation">
                <a title="Call For Papers" href="cfp">Call For Papers</a>
            </td>
            <td class="navigation">
                <a title="Organizers" href="organization">Organizers</a> 
            </td>
            <td class="navigation">
                <a title="Keynote" href="keynote">Keynote</a>
            </td>
            <td class="navigation">
                <a title="Accepted Papers" href="ap">Accepted Papers</a>
            </td>
            <td class="navigation">
                <a class="current" title="Schedule" href="schedule">Schedule</a>
            </td>
        </tr>
    </table>



<!--     <h2>To be announced.</h2> -->

    <h2>Attending Program</h2>
    <h2 style="font-size: 20px">2-6pm, Room 120, August 25th, 2024, <a href="https://ccib.es/"> Centre de Convencions Internacional de Barcelona</a>, Barcelona, Spain </h2>
    <table>
        <tr>
            <td class="date" rowspan="2">
                2:00 – 2:10 pm
            </td>
            <td>
                <strong>Opening Remarks</strong>
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="3">
                2:10 – 2:40 pm
            </td>
            <td>
                <strong>Keynote Talk 1: Toward Ethical AI for the Neglected Tails</strong> <br>
                <strong>Dr. James Caverlee</strong>, Texas A&M University <br>
                <u>Abstract</u>: In this talk, I will share some of our recent work centered on fairness and bias with an eye toward the “neglected tails” in applications like recommendation, LLMs, vision-language models, and speech systems. For example, these systems often demonstrate strong performance on popular concepts (or items or users), but in many cases there is a gap in the treatment of rare (or tail) concepts. Can we bridge this gap? These and similar questions are motivated by the Rawlsian max-min principle to design systems that maximize the least well-off (or “tails” of the distribution). 
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="3">
                2:40 – 3:40 pm
            </td>
            <td>
                <strong>Accepted Paper Talks 1 </strong>
                <ul>
                    <li style="list-style-type:square"> 
                        [Paper ID: 4] Practical Fairness: An Evaluation of Bias Mitigation Strategies for NLP Applications. <br>
                        <strong>Bryce King, Leidos</strong>
                    </li>
                    <li style="list-style-type:square"> 
                        [Paper ID: 5] Fair Data Generation via Score-based Diffusion Model. <br>
                        <strong>Minglai Shao, Tianjin University</strong>
                    </li>
                    <li style="list-style-type:square"> 
                        [Paper ID: 6] Matchings, Predictions and Counterfactual Harm in Refugee Resettlement Processes. <br>
                        <strong>Suhas Thejaswi, Max Planck Institute for Software Systems</strong>
                    </li>
                    <li style="list-style-type:square"> 
                        [Paper ID: 8] POSIT: Promotion of Semantic Item Tail via Adversarial Learning. <br>
                        <strong>Ding Tong, Netflix Research</strong>
                    </li>
                    <li style="list-style-type:square"> 
                        [Paper ID: 9] AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning. <br>
                        <strong>Maisha Binte Rashid, Baylor University</strong>
                    </li>
                </ul>
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                3:40 – 4:10 pm
            </td>
            <td>
                <strong>Keynote Talk 2: Trustworthy LLMs: Detection and Red-Teaming</strong> <br>
                <strong>Manish Nagireddy</strong>, IBM Research <br>
                <u>Abstract</u>: Trustworthy AI is paramount to the responsible use of AI systems. Despite the rising popularity of large language models (LLMs), their generative nature amplifies existing harms related to trust (such as fairness, robustness, transparency, etc.) and reveals new dangers (such as hallucinations, toxicity, etc.). I will first go through a catalog of harms and delve more deeply into how such harms can be automatically measured with detectors. Notably, these detectors can be applied throughout the LLM lifecycle (from filters on pre-training data to reward models during alignment to guardrails after deployment). Then, I will go through an example of developing a benchmark to capture a unique harm that was discovered via interactive probing. Next, I will combine both ideas to describe the development of a nuanced detector. Finally, I will end with future thoughts on the need for dynamic and participatory evaluation practices (such as red-teaming) and next steps for more trustworthy systems.
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="3">
                4:10 – 4:25 pm
            </td>
            <td>
                <strong>Coffee Break </strong>
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="3">
                4:25 – 4:55 pm
            </td>
            <td>
                <strong>Keynote Talk 3</strong> <br>
                <strong>Dr. Tyler Derr</strong>, Vanderbilt University
<!--                 <strong>Jian Kang (Presenter)</strong>, University of Illinois Urbana-Champaign<br> -->
<!--                 <u>Abstract</u>: Network (i.e., graph) mining plays a pivotal role in many high-impact application domains. State-of-the-art offers a wealth of sophisticated theories and algorithms, primarily focusing on answering who or what type question. On the other hand, the why or how question of network mining has not been well studied. For example, how can we ensure network mining is fair? How do mining results relate to the input graph topology? Why does the mining algorithm `think’ a transaction looks suspicious? In this talk, I will present our work on addressing individual fairness on graph mining.  First, we present a generic definition of individual fairness for graph mining which naturally leads to a quantitative measure of the potential bias in graph mining results. Second, we propose three mutually complementary algorithmic frameworks to mitigate the proposed individual bias measure, namely debiasing the input graph, debiasing the mining model and debiasing the mining results. Each algorithmic framework is formulated from the optimization perspective, using effective and efficient solvers, which are applicable to multiple graph mining tasks. Third, accommodating individual fairness is likely to change the original graph mining results without the fairness consideration. We develop an upper bound to characterize the cost (i.e., the difference between the graph mining results with and without the fairness consideration).  Toward the end of my talk, I will also introduce some other recent work on addressing the why & how question of network mining, and share my thoughts about the future work. -->

            </td>
        </tr>
    </table>
    
    <table>
        <tr>
            <td class="date" rowspan="3">
                4:55 – 5:55 pm
            </td>
            <td>
                <strong>Accepted Paper Talks 2</strong>
                <ul>
                    <li style="list-style-type:square"> 
                        [Paper ID: 11] Source Echo Chamber: Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop. <br>
                        <strong>Sunhao Dai, Renmin University of China</strong>
                    </li>
                    <li style="list-style-type:square"> 
                        [Paper ID: 12] OxonFair: A Flexible Toolkit for Algorithmic Fairness. <br>
                        <strong>Eoin Delaney, University of Oxford</strong>
                    </li>
                    <li style="list-style-type:square"> 
                        [Paper ID: 15] Enhancing Model Fairness and Accuracy with Similarity Networks: A Methodological Approach. <br>
                        <strong>Samira Maghool, University of Milan</strong>
                    </li>
                    <li style="list-style-type:square"> 
                        [Paper ID: 16] A Semidefinite Relaxation Approach for Fair Graph Clustering. <br>
                        <strong>Sina Baharlouei, eBay</strong>
                    </li>
                </ul>
            </td>
        </tr>
    </table>



    <table>
        <tr>
            <td class="date" rowspan="3">
                5:55 – 6:00 pm
            </td>
            <td>
                <strong>Closing</strong>
            </td>
        </tr>
    </table>






    <footer>
        Copyright &copy; 2024 All rights reserved.
    </footer>

</body>
</html>

